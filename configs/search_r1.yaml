
# SE Configuration for Search-R1 (QA Search & Reasoning)
# Task: Answer questions using multi-turn search and reasoning

# 任务类型
task_type: search_r1
metric_higher_is_better: true   # 新增：EM 评估，越大越好
# PerfAgent 使用的 LLM（本地 vLLM 推理，完全对齐 Search-R1 的 token 级续写）
# 设置 local_model_path 后无需启动 vLLM server，模型在进程内直接加载
model:
  name: SearchR1-nq_hotpotqa_train-qwen2.5-7b-it-em-grpo-v0.3
  # 本地 vLLM 推理：设为模型权重路径即启用（推荐）
  local_model_path: /data/user/rli112/junjie/workspace/LLM_Agent/weight/SearchR1-nq_hotpotqa_train-qwen2.5-7b-it-em-grpo-v0.3
  gpu_memory_utilization: 0.6
  tensor_parallel_size: 1
  max_model_len: 25000
  max_response_length: 5000
  # Chat API fallback（当 local_model_path 未设置时使用）
  api_base: http://127.0.0.1:8000/v1
  api_key: "EMPTY"
  max_input_tokens: 25000
  max_output_tokens: 5000
  temperature: 0.7

# 算子模型配置（用于 Plan/Reflection/Crossover/Summarizer 等算子层 LLM 调用）
# 这些算子不做检索推理，只做文本生成（策略规划、轨迹总结等）
# 需要启动 vLLM API server：bash scripts/launch_vllm_server.sh
# 注意：API server 和本地 vLLM 引擎需要在不同 GPU 上运行
operator_models:
  name: SearchR1-nq_hotpotqa_train-qwen2.5-7b-it-em-grpo-v0.3
  api_base: http://127.0.0.1:8000/v1
  api_key: "EMPTY"
  max_input_tokens: 25000
  max_output_tokens: 5000
  temperature: 0.7

operator_selection_mode: weighted

local_memory:
  enabled: false

global_memory_bank:
  enabled: false

# 实例配置
instances:
  instances_dir: instances/

output_dir: "trajectories_perf/search_r1_test_{timestamp}"

prompt_config:
  plan:
    system_prompt: |
      You are a QA search strategy planner. Given a factual question, generate K diverse search strategies.
      Each strategy should describe a DIFFERENT way to search for and find the answer, such as:
      - Direct entity search: search for the main entity directly
      - Relation chain: search step by step through relationships (e.g., find father, then find father's father)
      - Alternative naming: search using different names, aliases, or titles for the entity
      - Context search: search for related events, places, or time periods to find the answer indirectly
      - Decomposition: break a multi-hop question into sub-questions and search each separately

      Return ONLY a JSON code block:
      ```json
      {"strategies": ["strategy 1 description", "strategy 2 description", ...]}
      ```
      The array must contain exactly K strings. Each string should be 1-2 sentences describing the search approach.
      NO text outside the code block.
    user_prompt_template: |
      Generate {k} diverse search strategies for the following question:

      Question: {problem_text}

      Required Count: {k}
    strategy_header: |
      Below is a suggested search strategy. You may use it as guidance, but focus on answering the question correctly.
    fallback_patterns:
      - "Search directly for the main entity mentioned in the question to find basic facts."
      - "Break the question into sub-questions and search for each part separately."
      - "Search for the entity using alternative names, titles, or spellings."
      - "Search for related entities or events that might lead to the answer indirectly."
      - "Search for a broader category or list that would contain the answer."

  summarizer:
    enable_summary: true
    system_prompt: |
      You are an AI assistant specialized in analyzing search-augmented QA reasoning trajectories.

      The agent's goal is to answer a factual question correctly by searching for relevant information
      and reasoning step by step. The evaluation is binary: 1.0 if the answer matches the ground truth
      (Exact Match), 0.0 otherwise.

      The agent uses:
      - <think>...</think> to reason about the question and search results
      - <search>query</search> to search for information
      - <information>...</information> contains search results returned by the engine
      - <answer>...</answer> to provide the final answer

      Your goal is to summarize the agent's search and reasoning journey: what queries were tried,
      what information was found, how the reasoning evolved, and whether the final answer is correct.

      Return your analysis in JSON format with:
      - "solution_name": A short nickname for the search strategy used (e.g., "direct_search", "multi_hop_v2", "entity_chain")
      - "approach_summary": High-level narrative of the search and reasoning journey
      - "search_queries": List of search queries used and their effectiveness
      - "reasoning_quality": Assessment of the reasoning chain (logical, missed connections, etc.)
      - "answer_quality": Whether the final answer directly addresses the question
      - "key_learnings": Insights about effective search strategies for this type of question

    user_prompt_template: |
      Analyze the following search-augmented QA trajectory. The agent's goal was to answer the question
      correctly using search and reasoning (binary metric: Exact Match).

      Problem description:
      {problem_description}

      Trajectory data (.tra file):
      {trajectory_content}

      Final answer trajectory:
      {solution_content}

      Provide your analysis in the JSON format specified in the system prompt.

  reflection_refine:
    header: |
      IMPORTANT: Your previous attempt gave a WRONG answer. A condensed summary of the previous attempt is shown below.
      You MUST NOT repeat the same wrong answer. The answer shown below is INCORRECT.
      Try completely different search queries and reasoning to find the correct answer.
    guidelines: |
      Rules:
      1. The answer in the previous attempt is WRONG — NEVER give that same answer again.
      2. The information that led to that wrong answer may also be wrong — do NOT trust it.
      3. Use DIFFERENT search queries than what was tried before. The summary shows which queries were already tried — avoid repeating them.
      4. Search for each entity separately. Try alternative names, spellings, or related terms.
      5. Carefully check dates, names, and facts in the search results before concluding.
      6. Put your final answer inside <answer>...</answer> tags.

  crossover:
    header: |
      IMPORTANT: Condensed summaries of two previous attempts are shown below. BOTH gave WRONG answers.
      You MUST NOT repeat either wrong answer. Both answers shown below are INCORRECT.
      Use any useful clues from both attempts, but reach a DIFFERENT conclusion.
    guidelines: |
      Rules:
      1. Both answers below are WRONG — NEVER give either of them as your answer.
      2. The information that led to those wrong answers may be incorrect — verify everything.
      3. Review the search queries listed in the summaries — avoid repeating the same queries.
      4. Try NEW search queries to find information that both attempts missed. Use different keywords, alternative names, or more specific terms.
      5. Carefully check dates, names, and facts before concluding.
      6. Put your final answer inside <answer>...</answer> tags.

max_iterations: 1 # This config defines a single, multi-step iteration.

# 基础配置引用
base_config: "configs/perf_configs/config_search_r1.yaml"

# 策略编排
# 策略编排
strategy:
  iterations:
    # ==========================================================================
    # Initialization: Planner generates K=5 initial solutions
    # ==========================================================================
    - operator: "plan"
      num: 5
      trajectory_labels: ["iter1_sol1", "iter1_sol2", "iter1_sol3", "iter1_sol4", "iter1_sol5"]

    # ==========================================================================
    # Evolution: 10 hard-coded steps of exploration and exploitation
    # Operators will use weighted selection from the entire pool of trajectories.
    # ==========================================================================

    # Step 1: Reflection and Refine
    - operator: "reflection_refine"
      trajectory_label: "iter1_sol6"

    # Step 2: Crossover
    - operator: "crossover"
      trajectory_label: "iter1_sol7"

    # Step 3: Reflection and Refine
    - operator: "reflection_refine"
      trajectory_label: "iter1_sol8"

    # Step 4: Crossover
    - operator: "crossover"
      trajectory_label: "iter1_sol9"

    # Step 5: Reflection and Refine
    - operator: "reflection_refine"
      trajectory_label: "iter1_sol10"

    # Step 6: Crossover
    - operator: "crossover"
      trajectory_label: "iter1_sol11"

    # Step 7: Reflection and Refine
    - operator: "reflection_refine"
      trajectory_label: "iter1_sol12"

    # Step 8: Crossover
    - operator: "crossover"
      trajectory_label: "iter1_sol13"

    # Step 9: Reflection and Refine
    - operator: "reflection_refine"
      trajectory_label: "iter1_sol14"

    # Step 10: Crossover
    - operator: "crossover"
      trajectory_label: "iter1_sol15"

    # Iteration 2 
    # ==========================================================================
    # Evolution: 10 hard-coded steps of exploration and exploitation
    # ==========================================================================

    # Step 1: Reflection and Refine
    - operator: "reflection_refine"
      trajectory_label: "iter2_sol1"

    # Step 2: Crossover
    - operator: "crossover"
      trajectory_label: "iter2_sol2"

    # Step 3: Reflection and Refine
    - operator: "reflection_refine"
      trajectory_label: "iter2_sol3"

    # Step 4: Crossover
    - operator: "crossover"
      trajectory_label: "iter2_sol4"

    # Step 5: Reflection and Refine
    - operator: "reflection_refine"
      trajectory_label: "iter2_sol5"

    # Step 6: Crossover
    - operator: "crossover"
      trajectory_label: "iter2_sol6"

    # Step 7: Reflection and Refine
    - operator: "reflection_refine"
      trajectory_label: "iter2_sol7"

    # Step 8: Crossover
    - operator: "crossover"
      trajectory_label: "iter2_sol8"

    # Step 9: Reflection and Refine
    - operator: "reflection_refine"
      trajectory_label: "iter2_sol9"

    # Step 10: Crossover
    - operator: "crossover"
      trajectory_label: "iter2_sol10"