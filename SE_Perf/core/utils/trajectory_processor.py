#!/usr/bin/env python3

"""
SE Framework Trajectory Processor

ä¸ºSEæ¡†æ¶æä¾›è½¨è¿¹æ–‡ä»¶å¤„ç†åŠŸèƒ½ï¼Œåœ¨æ¯ä¸ªiterationåç”Ÿæˆç®€åŒ–çš„.traæ–‡ä»¶ã€‚
åŸºäºconverter_old.pyçš„é€»è¾‘ï¼Œé€‚é…SEæ¡†æ¶çš„ç›®å½•ç»“æ„ã€‚
"""

import json
import re
from pathlib import Path
from typing import Any

# å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¯¼å…¥é—®é¢˜
# from .se_logger import get_se_logger


class TrajectoryProcessor:
    """è½¨è¿¹æ–‡ä»¶å¤„ç†å™¨ï¼Œç”¨äºç”Ÿæˆç®€åŒ–çš„.traæ–‡ä»¶"""

    def __init__(self):
        """åˆå§‹åŒ–è½¨è¿¹å¤„ç†å™¨"""
        # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¯¼å…¥
        try:
            from .se_logger import get_se_logger

            self.logger = get_se_logger("trajectory_processor", emoji="ğŸ¬")
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†æ—¥å¿—
            import logging

            self.logger = logging.getLogger("trajectory_processor")
            self.logger.setLevel(logging.INFO)

    def _count_tokens(self, text: str) -> int:
        """ç®€å•çš„tokenè®¡æ•°è¿‘ä¼¼ç®—æ³•"""
        if not text or not isinstance(text, str):
            return 0
        # åŸºç¡€tokenè®¡æ•° - æŒ‰ç©ºç™½ç¬¦å’Œå¸¸è§æ ‡ç‚¹åˆ†å‰²
        tokens = re.findall(r"\b\w+\b", text.lower())
        return len(tokens)

    def _compress_search_content(self, text: str, max_info_chars: int = 200) -> str:
        """å‹ç¼©åŒ…å«æœç´¢è½¨è¿¹çš„æ–‡æœ¬å†…å®¹ã€‚

        å°† <information>...</information> å—æ›¿æ¢ä¸ºç®€çŸ­æ‘˜è¦ï¼Œ
        æˆªæ–­è¿‡é•¿çš„ <think> å—ï¼Œå¤§å¹…å‡å°‘ .tra æ–‡ä»¶ä½“ç§¯ã€‚

        Args:
            text: åŸå§‹æ–‡æœ¬å†…å®¹
            max_info_chars: <information> å—ä¿ç•™çš„æœ€å¤§å­—ç¬¦æ•°

        Returns:
            å‹ç¼©åçš„æ–‡æœ¬
        """
        if not text or "<information>" not in text:
            return text

        def _compress_info_block(match: re.Match) -> str:
            content = match.group(1)
            doc_count = len(re.findall(r"Doc \d+\(", content))
            char_count = len(content)
            if max_info_chars > 0 and char_count <= max_info_chars:
                return match.group(0)
            summary = f"[{doc_count} docs, {char_count} chars]"
            return f"<information>{summary}</information>"

        compressed = re.sub(
            r"<information>(.*?)</information>",
            _compress_info_block,
            text,
            flags=re.DOTALL,
        )

        # æˆªæ–­è¿‡é•¿çš„ <think> å—
        def _compress_think_block(match: re.Match) -> str:
            content = match.group(1)
            if len(content) <= 400:
                return match.group(0)
            head = content[:150].rstrip()
            tail = content[-150:].lstrip()
            return f"<think>{head}\n... [truncated {len(content)} chars] ...\n{tail}</think>"

        compressed = re.sub(
            r"<think>(.*?)</think>",
            _compress_think_block,
            compressed,
            flags=re.DOTALL,
        )

        return compressed

    def _truncate_text(self, text: str, first_percent: float = 0.2, last_percent: float = 0.1) -> str:
        """
        ä½¿ç”¨å­—ç¬¦ç™¾åˆ†æ¯”çº¦æŸæˆªæ–­æ–‡æœ¬å†…å®¹

        Args:
            text: è¦æˆªæ–­çš„æ–‡æœ¬
            first_percent: ä¿ç•™å¼€å¤´çš„ç™¾åˆ†æ¯”ï¼ˆé»˜è®¤20%ï¼‰
            last_percent: ä¿ç•™ç»“å°¾çš„ç™¾åˆ†æ¯”ï¼ˆé»˜è®¤10%ï¼‰

        Returns:
            æˆªæ–­åçš„æ–‡æœ¬
        """
        if not text or not isinstance(text, str):
            return text

        text_length = len(text)

        # åªå¯¹è¶³å¤Ÿé•¿çš„å†…å®¹è¿›è¡Œæˆªæ–­
        if text_length < 300:
            return text

        # è®¡ç®—é¦–éƒ¨é•¿åº¦ï¼ˆ20%ï¼Œçº¦æŸåœ¨30-150å­—ç¬¦ï¼‰
        first_length = int(text_length * first_percent)
        first_length = max(30, min(150, first_length))

        # è®¡ç®—å°¾éƒ¨é•¿åº¦ï¼ˆ10%ï¼Œçº¦æŸåœ¨30-100å­—ç¬¦ï¼‰
        last_length = int(text_length * last_percent)
        last_length = max(30, min(100, last_length))

        # æ£€æŸ¥æˆªæ–­æ˜¯å¦æœ‰æ„ä¹‰ï¼ˆä¿ç•™è¶…è¿‡80%æ—¶ä¸æˆªæ–­ï¼‰
        truncated_length = first_length + last_length + len("... [TRUNCATED] ...")
        if truncated_length >= text_length * 0.8:
            return text

        # æå–é¦–å°¾éƒ¨åˆ†
        first_part = text[:first_length]
        last_part = text[-last_length:]

        # ç»„åˆæˆªæ–­æ ‡è®°
        return f"{first_part}... [TRUNCATED] ...{last_part}"

    def _truncate_tool_content(self, content) -> str:
        """æˆªæ–­å·¥å…·è¾“å‡ºå†…å®¹"""
        if not content:
            return content

        # å¤„ç†åˆ—è¡¨æ ¼å¼: [{"type": "text", "text": "..."}]
        if isinstance(content, list) and len(content) > 0:
            first_item = content[0]
            if isinstance(first_item, dict) and "text" in first_item:
                text_content = first_item["text"]
                if isinstance(text_content, str):
                    return self._truncate_text(text_content)

        # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼
        if isinstance(content, str):
            return self._truncate_text(content)

        return content

    def _create_tra_from_traj(self, traj_file: Path, tra_file: Path) -> dict[str, int]:
        """
        ä».trajæ–‡ä»¶åˆ›å»º.traæ–‡ä»¶ï¼Œåªä¿ç•™historyçš„role/content

        Args:
            traj_file: åŸå§‹è½¨è¿¹æ–‡ä»¶è·¯å¾„
            tra_file: ç›®æ ‡.traæ–‡ä»¶è·¯å¾„

        Returns:
            å¤„ç†ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            with open(traj_file, encoding="utf-8") as f:
                traj_data = json.load(f)

            # æå–å¹¶ç®€åŒ– history
            history = traj_data.get("history", [])

            # perf è½¨è¿¹ï¼šè‹¥å†å²é¡¹åŒ…å« message_typeï¼Œåˆ™è§†ä¸º perfagent è½¨è¿¹ã€‚
            # ä»…ä¿ç•™ role ä¸ contentï¼Œä¸åšæˆªæ–­æˆ–å­—æ®µæ”¹å†™ã€‚
            is_perf_traj = any(isinstance(it, dict) and ("message_type" in it) for it in history)

            simplified_history = []
            total_tokens = 0
            original_tokens = 0  # åŸå§‹tokenæ•°ç»Ÿè®¡

            # perf è½¨è¿¹å¤„ç†
            if is_perf_traj:
                for item in history:
                    if not isinstance(item, dict) or "role" not in item:
                        continue

                    simplified_item = {"role": item["role"]}
                    if "content" in item:
                        content_val = item["content"]
                        # å¯¹åŒ…å«æœç´¢è½¨è¿¹çš„ content åšå‹ç¼©
                        # ï¼ˆå‹ç¼© <information> å—å’Œè¿‡é•¿çš„ <think> å—ï¼‰
                        if isinstance(content_val, str):
                            content_val = self._compress_search_content(content_val)
                        simplified_item["content"] = content_val
                        content_str = str(content_val) if content_val is not None else ""
                        tokens = self._count_tokens(content_str)
                        original_tokens += tokens
                        total_tokens += tokens

                    if len(simplified_item) > 1:
                        simplified_history.append(simplified_item)

                # åˆ›å»º.traæ–‡ä»¶å†…å®¹ï¼ˆä»… role/contentï¼‰
                tra_data = {"Trajectory": simplified_history}

                with open(tra_file, "w", encoding="utf-8") as f:
                    json.dump(tra_data, f, indent=2)

                return {
                    "total_tokens": total_tokens,
                    "original_tokens": original_tokens,
                    "saved_tokens": 0,
                    "compression_ratio": 0,
                    "history_items": len(simplified_history),
                }

            for item in history:
                if "role" not in item:
                    continue

                simplified_item = {"role": item["role"]}

                # é¦–å…ˆç»Ÿè®¡åŸå§‹å†…å®¹çš„tokenæ•°
                for field in ["content", "thought", "action"]:
                    if item.get(field):
                        original_field_str = str(item[field]) if item[field] else ""
                        original_tokens += self._count_tokens(original_field_str)

                # æ ¹æ®è§’è‰²ç±»å‹å¤„ç†ä¸åŒå­—æ®µ
                if item["role"] == "assistant":
                    # assistantè§’è‰²ï¼šæå–thoughtè€Œécontent
                    if item.get("thought"):
                        simplified_item["thought"] = item["thought"]

                    # åŒ…å«actionå¹¶åº”ç”¨æˆªæ–­
                    if item.get("action"):
                        original_action = item["action"]
                        action = original_action

                        # å¯¹str_replace_editoræˆ–é•¿action(>350å­—ç¬¦)åº”ç”¨æˆªæ–­
                        if isinstance(action, str):
                            if "str_replace_editor" in action or len(action) > 350:
                                action = self._truncate_text(action)
                        elif isinstance(action, dict):
                            action_str = str(action)
                            if "str_replace_editor" in action_str or len(action_str) > 350:
                                action = self._truncate_text(action_str)

                        simplified_item["action"] = action

                else:
                    # éassistantè§’è‰²ï¼šä½¿ç”¨content
                    if item.get("content"):
                        original_content = item["content"]
                        content = original_content

                        # å¯¹toolè§’è‰²çš„é•¿è§‚å¯Ÿç»“æœåº”ç”¨æˆªæ–­
                        if item["role"] == "tool":
                            content = self._truncate_tool_content(content)

                        simplified_item["content"] = content

                # åªæ·»åŠ æœ‰æ„ä¹‰å†…å®¹çš„é¡¹ï¼ˆä¸åªæ˜¯roleï¼‰
                if len(simplified_item) > 1:
                    simplified_history.append(simplified_item)

                    # ç»Ÿè®¡å‹ç¼©åå­—æ®µçš„tokenæ•°
                    for field in ["content", "thought", "action"]:
                        if field in simplified_item:
                            field_str = str(simplified_item[field]) if simplified_item[field] else ""
                            total_tokens += self._count_tokens(field_str)

            # åˆ›å»º.traæ–‡ä»¶å†…å®¹
            tra_data = {"Trajectory": simplified_history}

            # å†™å…¥.traæ–‡ä»¶
            with open(tra_file, "w", encoding="utf-8") as f:
                json.dump(tra_data, f, indent=2)

            # è®¡ç®—èŠ‚çœçš„tokenæ•°
            saved_tokens = original_tokens - total_tokens
            compression_ratio = (saved_tokens / original_tokens * 100) if original_tokens > 0 else 0

            return {
                "total_tokens": total_tokens,
                "original_tokens": original_tokens,
                "saved_tokens": saved_tokens,
                "compression_ratio": compression_ratio,
                "history_items": len(simplified_history),
            }

        except Exception as e:
            self.logger.error(f"åˆ›å»º.traæ–‡ä»¶å¤±è´¥ {traj_file}: {e}")
            return {
                "total_tokens": 0,
                "original_tokens": 0,
                "saved_tokens": 0,
                "compression_ratio": 0,
                "history_items": 0,
            }

    def process_iteration_directory(self, iteration_dir: Path) -> dict[str, Any]:
        """
        å¤„ç†å•ä¸ªiterationç›®å½•ï¼Œä¸ºæ‰€æœ‰å®ä¾‹ç”Ÿæˆ.traæ–‡ä»¶

        Args:
            iteration_dir: iterationç›®å½•è·¯å¾„ï¼ˆå¦‚iteration_1/ï¼‰

        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡ä¿¡æ¯
        """
        self.logger.info(f"å¼€å§‹å¤„ç†iterationç›®å½•: {iteration_dir}")

        if not iteration_dir.exists() or not iteration_dir.is_dir():
            self.logger.warning(f"ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {iteration_dir}")
            return {}

        processing_stats = {
            "iteration_dir": str(iteration_dir),
            "processed_instances": [],
            "total_tokens": 0,
            "total_tra_files": 0,
            "failed_instances": [],
        }

        # æ”¶é›†éœ€è¦å¤„ç†çš„ (traj_file, æ‰€åœ¨ç›®å½•, å®ä¾‹å) åˆ—è¡¨
        traj_entries: list[tuple[Path, Path, str]] = []

        # 1) å…ˆæ£€æŸ¥ iteration_dir ä¸‹ç›´æ¥å­˜åœ¨çš„ .traj æ–‡ä»¶ï¼ˆå•å®ä¾‹æ‰å¹³æ¨¡å¼ï¼‰
        direct_traj_files = list(iteration_dir.glob("*.traj"))
        for traj_file in direct_traj_files:
            traj_entries.append((traj_file, iteration_dir, traj_file.stem))

        # 2) å†éå†å­ç›®å½•ä¸­çš„ .traj æ–‡ä»¶ï¼ˆå¤šå®ä¾‹ / æ—§ç›®å½•ç»“æ„å…¼å®¹ï¼‰
        for instance_dir in iteration_dir.iterdir():
            if not instance_dir.is_dir() or instance_dir.name.startswith("."):
                continue
            for traj_file in instance_dir.glob("*.traj"):
                traj_entries.append((traj_file, instance_dir, instance_dir.name))

        if not traj_entries:
            self.logger.debug(f"è¿­ä»£ç›®å½• {iteration_dir.name} ä¸­æ²¡æœ‰ .traj æ–‡ä»¶")

        # æŒ‰å®ä¾‹ååˆ†ç»„å¤„ç†
        instances_map: dict[str, list[tuple[Path, Path]]] = {}
        for traj_file, parent_dir, inst_name in traj_entries:
            instances_map.setdefault(inst_name, []).append((traj_file, parent_dir))

        for inst_name, file_list in instances_map.items():
            instance_stats = {
                "instance_name": inst_name,
                "tra_files_created": [],
                "total_tokens": 0,
                "total_history_items": 0,
            }

            for traj_file, parent_dir in file_list:
                tra_file = parent_dir / (traj_file.stem + ".tra")

                # ç”Ÿæˆ.traæ–‡ä»¶
                file_stats = self._create_tra_from_traj(traj_file, tra_file)

                if file_stats["history_items"] > 0:
                    instance_stats["tra_files_created"].append(
                        {
                            "traj_file": traj_file.name,
                            "tra_file": tra_file.name,
                            "tokens": file_stats["total_tokens"],
                            "original_tokens": file_stats["original_tokens"],
                            "saved_tokens": file_stats["saved_tokens"],
                            "compression_ratio": file_stats["compression_ratio"],
                            "history_items": file_stats["history_items"],
                        }
                    )
                    instance_stats["total_tokens"] += file_stats["total_tokens"]
                    instance_stats["total_history_items"] += file_stats["history_items"]

                    self.logger.info(
                        f"å·²åˆ›å»º {tra_file.name}: {file_stats['history_items']} å†å²é¡¹, "
                        f"{file_stats['total_tokens']} tokens "
                        f"(åŸå§‹: {file_stats['original_tokens']}, "
                        f"èŠ‚çœ: {file_stats['saved_tokens']}, "
                        f"å‹ç¼©ç‡: {file_stats['compression_ratio']:.1f}%)"
                    )
                else:
                    processing_stats["failed_instances"].append(
                        {
                            "instance_name": inst_name,
                            "traj_file": traj_file.name,
                            "reason": "No valid history items",
                        }
                    )

            if instance_stats["tra_files_created"]:
                processing_stats["processed_instances"].append(instance_stats)
                processing_stats["total_tokens"] += instance_stats["total_tokens"]
                processing_stats["total_tra_files"] += len(instance_stats["tra_files_created"])

                self.logger.info(
                    f"å®ä¾‹ {inst_name}: åˆ›å»ºäº† {len(instance_stats['tra_files_created'])} ä¸ª.traæ–‡ä»¶"
                )

        # è®°å½•å¤„ç†ç»“æœ
        self.logger.info(
            f"iterationå¤„ç†å®Œæˆ: åˆ›å»ºäº† {processing_stats['total_tra_files']} ä¸ª.traæ–‡ä»¶, "
            f"æ€»è®¡ ~{processing_stats['total_tokens']} tokens"
        )

        if processing_stats["failed_instances"]:
            self.logger.warning(f"å¤±è´¥çš„å®ä¾‹æ•°: {len(processing_stats['failed_instances'])}")

        return processing_stats

    def process_workspace_directory(
        self, workspace_dir: Path, target_iterations: list[int] | None = None
    ) -> dict[str, Any]:
        """
        å¤„ç†æ•´ä¸ªworkspaceç›®å½•çš„æ‰€æœ‰iterations

        Args:
            workspace_dir: workspaceç›®å½•è·¯å¾„
            target_iterations: æŒ‡å®šè¦å¤„ç†çš„iterationåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰

        Returns:
            æ•´ä½“å¤„ç†ç»“æœç»Ÿè®¡
        """
        self.logger.info(f"å¼€å§‹å¤„ç†workspaceç›®å½•: {workspace_dir}")

        if not workspace_dir.exists() or not workspace_dir.is_dir():
            self.logger.error(f"Workspaceç›®å½•ä¸å­˜åœ¨: {workspace_dir}")
            return {}

        workspace_stats = {
            "workspace_dir": str(workspace_dir),
            "iterations_processed": [],
            "total_tokens": 0,
            "total_tra_files": 0,
            "processing_errors": [],
        }

        # æŸ¥æ‰¾æ‰€æœ‰iterationç›®å½•
        iteration_pattern = re.compile(r"^iteration_(\d+)$")
        iteration_dirs = []

        for item in workspace_dir.iterdir():
            if item.is_dir():
                match = iteration_pattern.match(item.name)
                if match:
                    iteration_num = int(match.group(1))
                    if target_iterations is None or iteration_num in target_iterations:
                        iteration_dirs.append((iteration_num, item))

        # æŒ‰iterationå·æ’åº
        iteration_dirs.sort(key=lambda x: x[0])

        if not iteration_dirs:
            self.logger.warning("æœªæ‰¾åˆ°ä»»ä½•iterationç›®å½•")
            return workspace_stats

        # å¤„ç†æ¯ä¸ªiteration
        for iteration_num, iteration_dir in iteration_dirs:
            try:
                iteration_stats = self.process_iteration_directory(iteration_dir)
                if iteration_stats:
                    workspace_stats["iterations_processed"].append(
                        {"iteration_number": iteration_num, "stats": iteration_stats}
                    )
                    workspace_stats["total_tokens"] += iteration_stats["total_tokens"]
                    workspace_stats["total_tra_files"] += iteration_stats["total_tra_files"]
            except Exception as e:
                error_info = {"iteration_number": iteration_num, "iteration_dir": str(iteration_dir), "error": str(e)}
                workspace_stats["processing_errors"].append(error_info)
                self.logger.error(f"å¤„ç†iteration_{iteration_num}æ—¶å‡ºé”™: {e}")

        # æœ€ç»ˆç»Ÿè®¡
        processed_iterations = len(workspace_stats["iterations_processed"])
        self.logger.info(
            f"Workspaceå¤„ç†å®Œæˆ: {processed_iterations} ä¸ªiteration, "
            f"{workspace_stats['total_tra_files']} ä¸ª.traæ–‡ä»¶, "
            f"~{workspace_stats['total_tokens']} tokens"
        )

        return workspace_stats

    def extract_problem_from_tra(self, tra_file: Path, problem_file: Path) -> bool:
        """
        ä».traæ–‡ä»¶ä¸­æå–problemæè¿°å¹¶ä¿å­˜ä¸º.problemæ–‡ä»¶

        Args:
            tra_file: .traæ–‡ä»¶è·¯å¾„
            problem_file: ç›®æ ‡.problemæ–‡ä»¶è·¯å¾„

        Returns:
            Trueè¡¨ç¤ºæˆåŠŸæå–ï¼ŒFalseè¡¨ç¤ºå¤±è´¥
        """
        try:
            with open(tra_file, encoding="utf-8") as f:
                tra_data = json.load(f)

            # å®šä½åˆ°Trajectory[1]["content"][0]["text"]
            trajectory = tra_data.get("Trajectory", [])
            if len(trajectory) < 2:
                self.logger.warning(f"traæ–‡ä»¶æ ¼å¼å¼‚å¸¸ï¼Œtrajectoryé•¿åº¦ä¸è¶³: {tra_file}")
                return False

            user_entry = trajectory[1]
            if user_entry.get("role") != "user":
                self.logger.warning(f"trajectory[1]ä¸æ˜¯userè§’è‰²: {tra_file}")
                return False

            content = user_entry.get("content", [])
            if not isinstance(content, list) or len(content) == 0:
                self.logger.warning(f"user contentæ ¼å¼å¼‚å¸¸: {tra_file}")
                return False

            text_content = content[0].get("text", "")
            if not text_content:
                self.logger.warning(f"æœªæ‰¾åˆ°textå†…å®¹: {tra_file}")
                return False

            # ä½¿ç”¨æ­£åˆ™æå–<pr_description>æ ‡ç­¾ä¸­çš„å†…å®¹
            import re

            match = re.search(r"<pr_description>\s*(.*?)\s*</pr_description>", text_content, re.DOTALL)
            if not match:
                self.logger.warning(f"æœªæ‰¾åˆ°pr_descriptionæ ‡ç­¾: {tra_file}")
                return False

            problem_description = match.group(1).strip()
            if not problem_description:
                self.logger.warning(f"pr_descriptionå†…å®¹ä¸ºç©º: {tra_file}")
                return False

            # å†™å…¥.problemæ–‡ä»¶
            with open(problem_file, "w", encoding="utf-8") as f:
                f.write(problem_description)

            # ç»Ÿè®¡ä¿¡æ¯
            problem_tokens = self._count_tokens(problem_description)
            self.logger.info(f"å·²æå–problem: {problem_file.name} ({problem_tokens} tokens)")

            return True

        except Exception as e:
            self.logger.error(f"æå–problemå¤±è´¥ {tra_file}: {e}")
            return False

    def process_problems_in_iteration(self, iteration_dir: Path) -> dict[str, Any]:
        """
        ä¸ºiterationç›®å½•ä¸­çš„æ‰€æœ‰å®ä¾‹æå–problemæ–‡ä»¶

        Args:
            iteration_dir: iterationç›®å½•è·¯å¾„

        Returns:
            æå–ç»“æœç»Ÿè®¡
        """
        self.logger.info(f"å¼€å§‹æå–iterationç›®å½•çš„problems: {iteration_dir}")

        if not iteration_dir.exists() or not iteration_dir.is_dir():
            self.logger.warning(f"ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {iteration_dir}")
            return {}

        problem_stats = {
            "iteration_dir": str(iteration_dir),
            "problems_extracted": [],
            "total_problems": 0,
            "failed_extractions": [],
        }

        # éå†æ‰€æœ‰å®ä¾‹ç›®å½•
        for instance_dir in iteration_dir.iterdir():
            if not instance_dir.is_dir() or instance_dir.name.startswith("."):
                continue

            # æŸ¥æ‰¾.traæ–‡ä»¶
            tra_files = list(instance_dir.glob("*.tra"))
            if not tra_files:
                self.logger.debug(f"å®ä¾‹ {instance_dir.name} æ²¡æœ‰.traæ–‡ä»¶")
                continue

            # å¤„ç†æ¯ä¸ª.traæ–‡ä»¶ï¼ˆé€šå¸¸åªæœ‰ä¸€ä¸ªï¼‰
            for tra_file in tra_files:
                problem_file = instance_dir / (instance_dir.name + ".problem")

                success = self.extract_problem_from_tra(tra_file, problem_file)

                if success:
                    problem_stats["problems_extracted"].append(
                        {
                            "instance_name": instance_dir.name,
                            "tra_file": tra_file.name,
                            "problem_file": problem_file.name,
                        }
                    )
                    problem_stats["total_problems"] += 1
                else:
                    problem_stats["failed_extractions"].append(
                        {
                            "instance_name": instance_dir.name,
                            "tra_file": tra_file.name,
                            "reason": "Problem extraction failed",
                        }
                    )

        self.logger.info(
            f"problemæå–å®Œæˆ: æˆåŠŸ {problem_stats['total_problems']} ä¸ª, "
            f"å¤±è´¥ {len(problem_stats['failed_extractions'])} ä¸ª"
        )

        return problem_stats


def process_trajectory_files(workspace_dir: str, iterations: list[int] | None = None) -> dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šå¤„ç†è½¨è¿¹æ–‡ä»¶

    Args:
        workspace_dir: workspaceç›®å½•è·¯å¾„
        iterations: è¦å¤„ç†çš„iterationåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰

    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    processor = TrajectoryProcessor()
    return processor.process_workspace_directory(Path(workspace_dir), iterations)


def extract_problems_from_workspace(workspace_dir: str, iterations: list[int] | None = None) -> dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä»workspaceæå–problemæ–‡ä»¶

    Args:
        workspace_dir: workspaceç›®å½•è·¯å¾„
        iterations: è¦å¤„ç†çš„iterationåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰

    Returns:
        æå–ç»“æœç»Ÿè®¡
    """
    import re

    processor = TrajectoryProcessor()
    workspace_path = Path(workspace_dir)

    if not workspace_path.exists():
        return {"error": f"Workspaceç›®å½•ä¸å­˜åœ¨: {workspace_path}"}

    # æŸ¥æ‰¾iterationç›®å½•
    iteration_pattern = re.compile(r"^iteration_(\d+)$")
    iteration_dirs = []

    for item in workspace_path.iterdir():
        if item.is_dir():
            match = iteration_pattern.match(item.name)
            if match:
                iteration_num = int(match.group(1))
                if iterations is None or iteration_num in iterations:
                    iteration_dirs.append((iteration_num, item))

    iteration_dirs.sort(key=lambda x: x[0])

    workspace_results = {
        "workspace_dir": str(workspace_path),
        "iterations_processed": [],
        "total_problems": 0,
        "total_failed": 0,
    }

    for iteration_num, iteration_dir in iteration_dirs:
        problem_stats = processor.process_problems_in_iteration(iteration_dir)
        if problem_stats:
            workspace_results["iterations_processed"].append(
                {"iteration_number": iteration_num, "stats": problem_stats}
            )
            workspace_results["total_problems"] += problem_stats.get("total_problems", 0)
            workspace_results["total_failed"] += len(problem_stats.get("failed_extractions", []))

    return workspace_results


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šå¤„ç†Demo_Structureç›®å½•
    demo_workspace = "/home/uaih3k9x/630_swe/SE/trajectories/Demo_Structure"

    processor = TrajectoryProcessor()
    results = processor.process_workspace_directory(Path(demo_workspace))

    print("å¤„ç†ç»“æœ:")
    print(f"- å¤„ç†çš„iterations: {len(results['iterations_processed'])}")
    print(f"- åˆ›å»ºçš„.traæ–‡ä»¶: {results['total_tra_files']}")
    print(f"- æ€»tokenæ•°: ~{results['total_tokens']}")
